{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "module7_title"
      },
      "source": [
        "# Module 7: Sentiment and Emotion Analysis Lab\n",
        "## ITAI 2373 - Natural Language Processing\n",
        "\n",
        "### Lab Overview\n",
        "Welcome to the Sentiment and Emotion Analysis lab! In this hands-on session, you'll build a complete emotion detection system that works with both text and speech. This lab connects directly to Module 7's concepts and gives you practical experience with real-world emotion analysis.\n",
        "\n",
        "\n",
        "### What You'll Build Today:\n",
        "1. **Text Sentiment Analyzer** using VADER and TextBlob\n",
        "2. **Machine Learning Classifier** with scikit-learn\n",
        "3. **Speech Emotion Detector** using audio features\n",
        "4. **Multimodal System** combining text and speech analysis\n",
        "\n",
        "### Real Data You'll Use:\n",
        "- Customer reviews from multiple domains\n",
        "- Social media posts with emotion labels\n",
        "- Audio recordings with emotional speech\n",
        "- Multimodal datasets combining text and audio\n",
        "\n",
        "### Learning Objectives:\n",
        "By the end of this lab, you will:\n",
        "- Understand the differences between rule-based and ML approaches to sentiment analysis\n",
        "- Build and evaluate multiple sentiment analysis systems\n",
        "- Extract and analyze emotional features from speech\n",
        "- Create a multimodal emotion detection system\n",
        "- Critically evaluate bias and fairness in emotion detection systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EytCn-8AA6wm"
      },
      "source": [
        "## Before we start let's see how this lab ties to previous modules ##\n",
        "\n",
        "---\n",
        "### üîó Building on Previous Modules\n",
        "\n",
        "Welcome to Module 7! This lab builds directly on everything you've learned so far:\n",
        "\n",
        "**From Module 1 (Introduction to NLP):**\n",
        "- We'll apply NLP to understand human emotions and opinions\n",
        "- Real-world applications: customer feedback, social media monitoring, healthcare\n",
        "\n",
        "**From Module 2 (Text Preprocessing):**\n",
        "- We'll use tokenization, normalization, and cleaning techniques\n",
        "- Preprocessing becomes crucial for emotion detection accuracy\n",
        "\n",
        "**From Module 3 (Audio and Preprocessing):**\n",
        "- We'll extract emotional features from speech signals\n",
        "- Combine text and audio for multimodal emotion analysis\n",
        "\n",
        "**From Module 4 (Text Representation):**\n",
        "- We'll use TF-IDF vectorization for machine learning approaches\n",
        "- Compare rule-based vs. ML feature representations\n",
        "\n",
        "**From Module 5 (Part-of-Speech Tagging):**\n",
        "- POS tags help identify emotional intensity (adjectives, adverbs)\n",
        "- Grammatical patterns reveal sentiment structure\n",
        "\n",
        "**From Module 6 (Syntax and Parsing):**\n",
        "- Dependency relationships show emotional targets (\"I love THIS product\")\n",
        "- Syntactic patterns help resolve sentiment scope and negation\n",
        "\n",
        "### üöÄNow,  Let's Get Started!\n",
        "Run each cell in order and complete the exercises marked with **‚úèÔ∏è YOUR TURN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## Part 0: Setup and Installation\n",
        "\n",
        "First, let's install all the libraries we'll need for this comprehensive lab. We'll be working with text processing, machine learning, and audio analysis libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AybZClVn_-jG"
      },
      "outputs": [],
      "source": [
        "# Install required libraries for Google Colab\n",
        "!pip install vaderSentiment textblob librosa soundfile\n",
        "!python -m textblob.download_corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_libraries"
      },
      "outputs": [],
      "source": [
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio, display\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All libraries installed and imported successfully!\")\n",
        "print(\"üìö Ready to start building emotion detection systems!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz7yFcReDH36"
      },
      "source": [
        "## üîó Let's Review in Practice How Previous Modules Tie in to Today:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "module_connections"
      },
      "outputs": [],
      "source": [
        "# Example text that demonstrates concepts from all previous modules\n",
        "example_text = \"I absolutely LOVE this amazing product! It works perfectly and exceeded my expectations.\"\n",
        "\n",
        "print(\"üîç Analyzing with Previous Module Concepts:\")\n",
        "print(f\"Original text: {example_text}\")\n",
        "print()\n",
        "\n",
        "# Module 2: Text Preprocessing\n",
        "import re\n",
        "import string\n",
        "\n",
        "def basic_preprocess(text):\n",
        "    # Tokenization (Module 2)\n",
        "    tokens = text.split()\n",
        "    # Normalization (Module 2)\n",
        "    tokens = [token.lower().strip(string.punctuation) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "tokens = basic_preprocess(example_text)\n",
        "print(f\"üìù Module 2 (Preprocessing) - Tokens: {tokens}\")\n",
        "\n",
        "# Module 4: Text Representation (TF-IDF preview)\n",
        "sample_docs = [\n",
        "    \"I love this product\",\n",
        "    \"This product is amazing\",\n",
        "    \"I hate this terrible product\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(sample_docs)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(f\"\\nüî¢ Module 4 (Text Representation) - TF-IDF Features: {list(feature_names)}\")\n",
        "\n",
        "# Module 5: POS Tagging for sentiment\n",
        "from textblob import TextBlob\n",
        "blob = TextBlob(example_text)\n",
        "pos_tags = blob.tags\n",
        "\n",
        "print(f\"\\nüè∑Ô∏è Module 5 (POS Tagging) - Tags: {pos_tags}\")\n",
        "print(\"   Notice: Adjectives (JJ) often carry emotional weight: 'amazing', 'perfect'\")\n",
        "\n",
        "# Module 6: Syntax for sentiment scope\n",
        "print(f\"\\nüïµÔ∏è Module 6 (Syntax/Parsing) - Dependency relationships help us understand:\")\n",
        "print(\"   - What is being loved? (the product)\")\n",
        "print(\"   - What makes it amazing? (it works perfectly)\")\n",
        "print(\"   - Scope of sentiment: entire product experience\")\n",
        "\n",
        "print(\"\\nüí° Key Insight: Sentiment analysis combines ALL previous concepts!\")\n",
        "print(\"   - Preprocessing cleans the text\")\n",
        "print(\"   - POS tags identify emotional words\")\n",
        "print(\"   - Syntax shows what the emotions target\")\n",
        "print(\"   - Text representation enables ML approaches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6SeDZvuGR39"
      },
      "source": [
        "### üìö **Let's Recap some of the foundation from EDA & Machine Learning DataFrames Quick Review**\n",
        "\n",
        "#### **What is a DataFrame?**\n",
        "\n",
        "Think of a **DataFrame** as a digital spreadsheet or table, just like Excel. It has:\n",
        "- **Rows** (horizontal) - each row represents one piece of data\n",
        "- **Columns** (vertical) - each column represents a different attribute or feature  \n",
        "- **Cells** - where specific data values are stored\n",
        "\n",
        "In Python, we use **pandas** (imported as `pd`) to create and work with DataFrames.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step-by-Step Breakdown of Our Sample Data Creation:**\n",
        "\n",
        "##### **Step 1: Creating the Text Data**\n",
        "```python\n",
        "sample_texts = [\n",
        "    \"I absolutely love this product! It's amazing and works perfectly.\",\n",
        "    \"This is the worst purchase I've ever made. Completely disappointed.\",\n",
        "    # ... more examples\n",
        "]\n",
        "```\n",
        "\n",
        "**What's happening here:**\n",
        "- We create a **list** (think of it as a container) of text examples\n",
        "- Each text represents what a real customer might write in a review or social media post\n",
        "- The square brackets `[]` define a list in Python\n",
        "- Each text is in quotes because it's a **string** (text data type)\n",
        "\n",
        "**Why these specific texts:**\n",
        "- **Positive examples**: \"I absolutely love this product!\" (clearly happy/positive)\n",
        "- **Negative examples**: \"This is the worst purchase!\" (clearly angry/negative)  \n",
        "- **Neutral examples**: \"The product is okay\" (neither positive nor negative)\n",
        "- **Social media style**: \"OMG this is INCREDIBLE!!! üòç\" (with emojis and excitement)\n",
        "\n",
        "##### **Step 2: Creating the DataFrame Structure**\n",
        "```python\n",
        "df_sample = pd.DataFrame({\n",
        "    'text': sample_texts,\n",
        "    'source': ['review', 'review', 'social_media', ...]\n",
        "})\n",
        "```\n",
        "\n",
        "**What's happening here:**\n",
        "- `pd.DataFrame({...})` creates a new table/spreadsheet\n",
        "- The curly braces `{}` define a **dictionary** - a way to pair names with values\n",
        "- `'text': sample_texts` creates a column called \"text\" and fills it with our text examples\n",
        "- `'source': [...]` creates a column called \"source\" and labels each text as either \"review\" or \"social_media\"\n",
        "\n",
        "**The resulting table looks like this:**\n",
        "\n",
        "| Index | text | source |\n",
        "|-------|------|--------|\n",
        "| 0 | \"I absolutely love this product!...\" | review |\n",
        "| 1 | \"This is the worst purchase...\" | review |\n",
        "| 2 | \"The product is okay...\" | review |\n",
        "| 3 | \"OMG this is INCREDIBLE!!!\" | social_media |\n",
        "| 4 | \"Meh... it's fine I guess...\" | review |\n",
        "\n",
        "---\n",
        "\n",
        "#### **üéØ Key Concepts Revisited:**\n",
        "\n",
        "1. **Lists** `[]`: Store multiple items in order\n",
        "2. **Dictionaries** `{}`: Pair names with values (like \"name\": \"John\")\n",
        "3. **DataFrames**: Tables for organizing data with rows and columns\n",
        "4. **Columns**: Vertical data categories (like \"text\" and \"source\")\n",
        "5. **Rows**: Horizontal data entries (each customer comment)\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîç Why Create This Sample Data?**\n",
        "\n",
        "##### **1. Controlled Testing Environment**\n",
        "- We know exactly what emotions each text should express\n",
        "- We can test if our emotion detection systems work correctly\n",
        "- Like having an answer key to check our work\n",
        "\n",
        "##### **2. Diverse Text Types**\n",
        "- **Product reviews**: Professional, detailed feedback\n",
        "- **Social media posts**: Casual, with emojis and slang\n",
        "- This tests how well our systems handle different writing styles\n",
        "\n",
        "##### **3. Balanced Emotions**\n",
        "- Mix of positive, negative, and neutral sentiments\n",
        "- Ensures our system can detect all types of emotions\n",
        "- Prevents bias toward one emotion type\n",
        "\n",
        "##### **4. Real-World Simulation**\n",
        "- These texts represent what you'd actually find online\n",
        "- Helps us understand how emotion detection works in practice\n",
        "- Prepares us for analyzing real customer feedback\n",
        "\n",
        "---\n",
        "\n",
        "#### **üöÄ Why This Matters for Emotion Analysis:**\n",
        "\n",
        "This sample data serves as our **training ground** where we can:\n",
        "- Test different emotion detection methods\n",
        "- Compare their accuracy\n",
        "- Understand their strengths and weaknesses\n",
        "- Learn how to handle different types of text\n",
        "\n",
        "Think of it like practicing basketball shots on a practice court before playing in a real game - we need controlled examples before analyzing real customer feedback!\n",
        "\n",
        "---\n",
        "\n",
        "#### **üìä Understanding the Output Code:**\n",
        "\n",
        "```python\n",
        "print(f\"Total texts: {len(df_sample)}\")\n",
        "print(f\"Sources: {df_sample['source'].value_counts().to_dict()}\")\n",
        "```\n",
        "\n",
        "**What each line does:**\n",
        "- `len(df_sample)` counts how many rows (text examples) we have\n",
        "- `df_sample['source'].value_counts()` counts how many of each source type we have\n",
        "- `.to_dict()` converts the count into a dictionary format for easy reading\n",
        "\n",
        "This gives us a quick summary of our data structure and helps us verify everything was created correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_sample_data"
      },
      "outputs": [],
      "source": [
        "# Create sample data that demonstrates different linguistic phenomena\n",
        "sample_texts = [\n",
        "    \"I absolutely love this product! It's amazing and works perfectly.\",  # Positive with intensifiers\n",
        "    \"This is the worst purchase I've ever made. Completely disappointed.\",  # Negative with superlatives\n",
        "    \"The product is okay, nothing special but does the job.\",  # Neutral with mixed signals\n",
        "    \"OMG this is INCREDIBLE!!! üòç Best thing ever!!!\",  # Social media style\n",
        "    \"I don't hate this product, but it's not great either.\",  # Negation (Module 6 syntax!)\n",
        "    \"The customer service was not bad, actually quite helpful.\",  # Double negation\n",
        "    \"This expensive product should work better for the price.\",  # Implicit criticism\n",
        "    \"Fast shipping, good quality, would recommend to others.\",  # Multiple aspects\n",
        "    \"It's fine I guess... could be worse üòê\",  # Lukewarm sentiment\n",
        "    \"TERRIBLE quality!!! Waste of money üò° Never buying again!\",  # Strong negative\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_sample = pd.DataFrame({\n",
        "    'text': sample_texts,\n",
        "    'linguistic_feature': [\n",
        "        'intensifiers', 'superlatives', 'mixed_signals', 'social_media',\n",
        "        'negation', 'double_negation', 'implicit', 'multi_aspect',\n",
        "        'lukewarm', 'strong_negative'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"üìù Sample Data with Linguistic Challenges:\")\n",
        "for i, row in df_sample.iterrows():\n",
        "    print(f\"{i+1}. [{row['linguistic_feature']}] {row['text']}\")\n",
        "\n",
        "print(\"üìù Sample Data Created:\")\n",
        "print(f\"Total texts: {len(df_sample)}\")\n",
        "print(f\"Sources: {df_sample['source'].value_counts().to_dict()}\")\n",
        "print(\"\\nüîç First few examples:\")\n",
        "for i, row in df_sample.head(3).iterrows():\n",
        "    print(f\"{i+1}. [{row['source']}] {row['text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_theory"
      },
      "source": [
        "## Part 1: Text Sentiment Analysis with VADER and TextBlob\n",
        "\n",
        "### Understanding Rule-Based Sentiment Analysis\n",
        "\n",
        "Rule-based sentiment analysis uses **lexicons** (dictionaries) that map words to sentiment scores. This connects directly to Module 4's discussion of text representation - instead of TF-IDF vectors, we use pre-built emotional dictionaries.\n",
        "\n",
        "**VADER (Valence Aware Dictionary and sEntiment Reasoner):**\n",
        "- Designed for social media text\n",
        "- Handles emojis, capitalization, punctuation\n",
        "- Uses linguistic rules (like Module 6's parsing rules)\n",
        "\n",
        "**TextBlob:**\n",
        "- General-purpose sentiment analysis\n",
        "- Provides both polarity and subjectivity scores\n",
        "- Based on movie review corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vader_analysis"
      },
      "source": [
        "### 1.1 VADER Sentiment Analysis\n",
        "\n",
        "**VADER (Valence Aware Dictionary and sEntiment Reasoner)** is specifically designed for social media text. It:\n",
        "- Handles emojis, slang, and intensifiers\n",
        "- Provides compound scores from -1 (most negative) to +1 (most positive)\n",
        "- Gives detailed breakdowns (positive, negative, neutral)\n",
        "\n",
        "Let's see how VADER analyzes our sample texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vader_implementation"
      },
      "outputs": [],
      "source": [
        "# Initialize VADER analyzer\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def analyze_with_vader(text):\n",
        "    \"\"\"Analyze sentiment using VADER\"\"\"\n",
        "    scores = vader_analyzer.polarity_scores(text)\n",
        "    return scores\n",
        "\n",
        "def classify_vader_sentiment(compound_score):\n",
        "    \"\"\"Convert VADER compound score to sentiment label\"\"\"\n",
        "    if compound_score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif compound_score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Analyze all sample texts with VADER\n",
        "vader_results = []\n",
        "for text in df_sample['text']:\n",
        "    scores = analyze_with_vader(text)\n",
        "    vader_results.append({\n",
        "        'compound': scores['compound'],\n",
        "        'positive': scores['pos'],\n",
        "        'negative': scores['neg'],\n",
        "        'neutral': scores['neu'],\n",
        "        'sentiment': classify_vader_sentiment(scores['compound'])\n",
        "    })\n",
        "\n",
        "# Add VADER results to DataFrame\n",
        "vader_df = pd.DataFrame(vader_results)\n",
        "df_sample = pd.concat([df_sample, vader_df.add_prefix('vader_')], axis=1)\n",
        "\n",
        "print(\"üéØ VADER Analysis Results:\")\n",
        "print(\"\\nüìä Sentiment Distribution:\")\n",
        "print(df_sample['vader_sentiment'].value_counts())\n",
        "\n",
        "print(\"\\nüîç Detailed Results:\")\n",
        "for i, row in df_sample.iterrows():\n",
        "    print(f\"{i+1}. {row['vader_sentiment']} (score: {row['vader_compound']:.3f})\")\n",
        "    print(f\"   Text: {row['text'][:60]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "textblob_analysis"
      },
      "source": [
        "### 1.2 TextBlob Sentiment Analysis\n",
        "\n",
        "**TextBlob** provides a different approach to sentiment analysis:\n",
        "- Uses a different lexicon and algorithm than VADER\n",
        "- Provides polarity (-1 to +1) and subjectivity (0 to 1) scores\n",
        "- Generally more conservative in its sentiment assignments\n",
        "\n",
        "Let's compare TextBlob results with VADER:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "textblob_implementation"
      },
      "outputs": [],
      "source": [
        "# TextBlob Analysis\n",
        "def analyze_with_textblob(text):\n",
        "    \"\"\"Analyze sentiment using TextBlob\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    return {\n",
        "        'polarity': blob.sentiment.polarity,\n",
        "        'subjectivity': blob.sentiment.subjectivity\n",
        "    }\n",
        "\n",
        "def classify_textblob_sentiment(polarity):\n",
        "    \"\"\"Convert TextBlob polarity to sentiment label\"\"\"\n",
        "    if polarity > 0.1:\n",
        "        return 'Positive'\n",
        "    elif polarity < -0.1:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Analyze with TextBlob\n",
        "textblob_results = []\n",
        "for text in df_sample['text']:\n",
        "    scores = analyze_with_textblob(text)\n",
        "    textblob_results.append({\n",
        "        'polarity': scores['polarity'],\n",
        "        'subjectivity': scores['subjectivity'],\n",
        "        'sentiment': classify_textblob_sentiment(scores['polarity'])\n",
        "    })\n",
        "\n",
        "# Add TextBlob results\n",
        "textblob_df = pd.DataFrame(textblob_results)\n",
        "df_sample = pd.concat([df_sample, textblob_df.add_prefix('textblob_')], axis=1)\n",
        "\n",
        "print(\"üéØ VADER vs TextBlob Comparison:\")\n",
        "comparison = pd.crosstab(df_sample['vader_sentiment'], df_sample['textblob_sentiment'], margins=True)\n",
        "print(comparison)\n",
        "\n",
        "print(\"\\nüîç Detailed Comparison:\")\n",
        "for i, row in df_sample.iterrows():\n",
        "    if row['vader_sentiment'] != row['textblob_sentiment']:\n",
        "        print(f\"DISAGREEMENT on [{row['linguistic_feature']}]:\")\n",
        "        print(f\"  Text: {row['text']}\")\n",
        "        print(f\"  VADER: {row['vader_sentiment']} ({row['vader_compound']:.3f})\")\n",
        "        print(f\"  TextBlob: {row['textblob_sentiment']} ({row['textblob_polarity']:.3f})\")\n",
        "        print()\n",
        "\n",
        "print(\"üéØ TextBlob Analysis Results:\")\n",
        "print(\"\\nüìä Sentiment Distribution:\")\n",
        "print(df_sample['textblob_sentiment'].value_counts())\n",
        "\n",
        "print(\"\\nüîç Comparison with VADER:\")\n",
        "comparison = pd.crosstab(df_sample['vader_sentiment'], df_sample['textblob_sentiment'], margins=True)\n",
        "print(comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_1"
      },
      "source": [
        "### ‚úèÔ∏è YOUR TURN - Exercise 1: Analyzing Different Text Types\n",
        "\n",
        "Now it's your turn to experiment with VADER and TextBlob! Complete the following tasks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iQcbPUfPgox"
      },
      "outputs": [],
      "source": [
        "# ‚úèÔ∏è YOUR TURN: Create examples from YOUR experience\n",
        "\n",
        "# 1. Think of 5 real examples from YOUR life where sentiment might be ambiguous\n",
        "# These should be based on:\n",
        "# - Something you actually said or wrote\n",
        "# - A review you read that confused you\n",
        "# - Social media posts from your friends/family\n",
        "# - Text messages where tone was misunderstood\n",
        "# - Cultural expressions from your background\n",
        "\n",
        "your_real_examples = [\n",
        "    # TODO: Replace with YOUR actual examples\n",
        "    \"Example 1: Something you actually said/wrote\",\n",
        "    \"Example 2: A confusing review you encountered\",\n",
        "    \"Example 3: A social media post that was misunderstood\",\n",
        "    \"Example 4: A text message where tone was unclear\",\n",
        "    \"Example 5: A cultural expression from your background\"\n",
        "]\n",
        "\n",
        "# 2. Analyze YOUR examples\n",
        "print(\"üß™ Analysis of YOUR Real-World Examples:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, text in enumerate(your_real_examples, 1):\n",
        "    # TODO: Analyze each with VADER and TextBlob\n",
        "    vader_scores = analyze_with_vader(text)\n",
        "    textblob_scores = analyze_with_textblob(text)\n",
        "\n",
        "    print(f\"\\n{i}. Your Example: {text}\")\n",
        "    print(f\"   VADER: {classify_vader_sentiment(vader_scores['compound'])} ({vader_scores['compound']:.3f})\")\n",
        "    print(f\"   TextBlob: {classify_textblob_sentiment(textblob_scores['polarity'])} ({textblob_scores['polarity']:.3f})\")\n",
        "\n",
        "# 3. Reflection Questions (AI cannot answer these for you!)\n",
        "print(\"\\nü§î Personal Reflection Questions:\")\n",
        "print(\"Answer these based on YOUR experience running the code above:\")\n",
        "print()\n",
        "print(\"1. Which of YOUR examples did the algorithms get wrong? Why do you think that happened?\")\n",
        "print(\"   Your answer: \")\n",
        "print()\n",
        "print(\"2. Did any results surprise you? Which ones and why?\")\n",
        "print(\"   Your answer: \")\n",
        "print()\n",
        "print(\"3. How did your cultural background or personal communication style affect the results?\")\n",
        "print(\"   Your answer: \")\n",
        "print()\n",
        "print(\"4. If you were to improve these algorithms, what would you focus on based on YOUR examples?\")\n",
        "print(\"   Your answer: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydP3IQyvPqPC"
      },
      "source": [
        "### üß† Conceptual Understanding Check\n",
        "\n",
        "**Answer these questions to cement your learning:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka1eDvCBPvBz"
      },
      "source": [
        "**Q1: How does sentiment analysis connect to Module 6 (Syntax/Parsing)?**\n",
        "\n",
        "*Your answer:*\n",
        "\n",
        "**Q2: Why might POS tags from Module 5 be useful for sentiment analysis?**\n",
        "\n",
        "*Your answer:*\n",
        "\n",
        "**Q3: How does text preprocessing from Module 2 affect sentiment analysis accuracy?**\n",
        "\n",
        "*Your answer:*\n",
        "\n",
        "**Q4: Compare rule-based sentiment analysis to the TF-IDF approach from Module 4. What are the key differences?**\n",
        "\n",
        "*Your answer:*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml_section"
      },
      "source": [
        "## Part 2: Machine Learning Approach to Sentiment Analysis\n",
        "\n",
        "### Moving Beyond Rule-Based Methods\n",
        "\n",
        "While rule-based methods like VADER and TextBlob are excellent for general-purpose sentiment analysis, a machine learning approach offers several key advantages. By training a model on a specific dataset, we can:\n",
        "- **Learn domain-specific patterns** from your data\n",
        "- **Handle context better** through feature learning\n",
        "- **Adapt to specific use cases** like product reviews vs. social media\n",
        "- **Potentially achieve higher accuracy** on your specific dataset\n",
        "\n",
        "To use a machine learning algorithm for a task like sentiment classification, we must first convert our text data into a numerical format that the model can understand. This process is called feature engineering or text representation.\n",
        "\n",
        "Let's build a machine learning classifier using scikit-learn and compare it with our rule-based methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cd9a816"
      },
      "source": [
        "### Loading Data for Machine Learning\n",
        "\n",
        "In a real-world machine learning sentiment analysis project, you would typically load a much larger dataset with a minimum of thousands or even millions of tokens. This dataset would likely be loaded from a file (like a CSV or JSON) or an API. The larger the dataset, the better the model can learn complex patterns and generalize to new, unseen text.\n",
        "\n",
        "**Note:** For the purpose of this exercise, we will be using a short, illustrative text dataset to demonstrate the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_ml_data"
      },
      "outputs": [],
      "source": [
        "# Create ML dataset with examples that test different linguistic phenomena\n",
        "ml_texts = [\n",
        "    # Positive examples with different structures\n",
        "    \"This product exceeded my expectations! Highly recommend.\",  # Simple positive\n",
        "    \"Amazing quality and fast shipping. Will buy again.\",  # Multiple positive aspects\n",
        "    \"Love this! Works perfectly and looks great.\",  # Enthusiastic\n",
        "    \"Not bad at all, actually quite good for the price.\",  # Positive via negation\n",
        "    \"Better than I thought it would be, surprisingly good.\",  # Exceeded expectations\n",
        "    \"Outstanding quality for the price. Very satisfied.\",  # Value-based positive\n",
        "    \"This is exactly what I was looking for. Perfect!\",  # Expectation match\n",
        "    \"Incredible value and fantastic performance.\",  # Superlatives\n",
        "    \"Couldn't be happier with this purchase.\",  # Negative construction, positive meaning\n",
        "    \"Superb quality and arrived quickly.\",  # Multiple positive attributes\n",
        "\n",
        "    # Negative examples with different structures\n",
        "    \"Terrible quality. Broke after one day.\",  # Simple negative\n",
        "    \"Worst customer service ever. Very disappointed.\",  # Superlative negative\n",
        "    \"Complete waste of money. Don't buy this.\",  # Strong negative advice\n",
        "    \"Not what I expected, quite disappointing really.\",  # Expectation mismatch\n",
        "    \"Arrived damaged and took forever to ship.\",  # Multiple negative aspects\n",
        "    \"Doesn't work as advertised. Very frustrated.\",  # Functional failure\n",
        "    \"Cheap plastic that feels like it will break.\",  # Quality criticism\n",
        "    \"Overpriced for such poor quality.\",  # Value-based negative\n",
        "    \"Regret buying this. Total disappointment.\",  # Emotional regret\n",
        "    \"Horrible experience from start to finish.\",  # Comprehensive negative\n",
        "\n",
        "    # Neutral examples with different structures\n",
        "    \"The product is okay. Nothing special.\",  # Simple neutral\n",
        "    \"Average quality for the price point.\",  # Expectation match\n",
        "    \"It works but could be better.\",  # Functional but limited\n",
        "    \"Decent product, meets basic requirements.\",  # Adequate performance\n",
        "    \"Not bad, not great. Just average.\",  # Explicit neutrality\n",
        "    \"Standard quality, as expected.\",  # Expectation match\n",
        "    \"It's fine for what it is.\",  # Conditional acceptance\n",
        "    \"Acceptable quality, nothing remarkable.\",  # Adequate but unremarkable\n",
        "    \"Does the job but nothing more.\",  # Functional adequacy\n",
        "    \"Mediocre product with basic features.\"  # Below average but functional\n",
        "]\n",
        "\n",
        "ml_labels = (\n",
        "    ['positive'] * 10 +\n",
        "    ['negative'] * 10 +\n",
        "    ['neutral'] * 10\n",
        ")\n",
        "\n",
        "# Create DataFrame\n",
        "df_ml = pd.DataFrame({\n",
        "    'text': ml_texts,\n",
        "    'sentiment': ml_labels\n",
        "})\n",
        "\n",
        "print(\"üìä Machine Learning Dataset:\")\n",
        "print(f\"Total examples: {len(df_ml)}\")\n",
        "print(f\"Label distribution: {df_ml['sentiment'].value_counts().to_dict()}\")\n",
        "\n",
        "# Show connection to Module 4 concepts\n",
        "print(\"\\nüîó Connection to Module 4 (Text Representation):\")\n",
        "print(\"- Each text will become a TF-IDF vector (like Module 4)\")\n",
        "print(\"- But now we have LABELS (sentiment) for supervised learning\")\n",
        "print(\"- This transforms unsupervised representation into supervised classification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature_extraction"
      },
      "source": [
        "### 2.1 Feature Extraction with TF-IDF\n",
        "\n",
        "Remember before we can train a machine learning model, we need to convert our text into numerical features. We'll use **TF-IDF (Term Frequency-Inverse Document Frequency)** which:\n",
        "- Captures the importance of words in documents\n",
        "- Reduces the impact of common words\n",
        "- Creates sparse but meaningful feature vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfidf_extraction"
      },
      "outputs": [],
      "source": [
        "# Feature extraction using TF-IDF (Module 4 concepts)\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_ml['text'],\n",
        "    df_ml['sentiment'],\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=df_ml['sentiment']\n",
        ")\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=1000,    # Limit vocabulary size\n",
        "    stop_words='english',  # Remove common English stop words\n",
        "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
        "    lowercase=True\n",
        ")\n",
        "\n",
        "\n",
        "# Fit the vectorizer on training data and transform both sets\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(\"üî¢ TF-IDF Feature Extraction (Module 4 Review):\")\n",
        "print(f\"Training matrix shape: {X_train_tfidf.shape}\")\n",
        "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
        "print(f\"Feature sparsity: {(1 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])):.3f}\")\n",
        "\n",
        "# Show some features\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(f\"\\nSample features: {list(feature_names[:15])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_training"
      },
      "source": [
        "### 2.2 Training Multiple Classifiers\n",
        "\n",
        "Let's train and compare different machine learning algorithms:\n",
        "- **Logistic Regression**: Linear model, fast and interpretable\n",
        "- **Random Forest**: Ensemble method, handles non-linear patterns\n",
        "\n",
        "We'll evaluate their performance and see how they compare to our rule-based methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_classifiers"
      },
      "outputs": [],
      "source": [
        "# Train Logistic Regression\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Train Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "lr_predictions = lr_model.predict(X_test_tfidf)\n",
        "rf_predictions = rf_model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate accuracies\n",
        "lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "\n",
        "print(\"üéØ Model Performance:\")\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy:.3f}\")\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy:.3f}\")\n",
        "\n",
        "# Detailed classification reports\n",
        "print(\"\\nüìä Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "\n",
        "print(\"\\nüìä Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, rf_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_interpretation"
      },
      "source": [
        "### 2.3 Model Interpretation\n",
        "\n",
        "Let's understand what our models have learned by examining the most important features for each sentiment class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interpret_models"
      },
      "outputs": [],
      "source": [
        "# Get feature importance from Logistic Regression\n",
        "def get_top_features(model, vectorizer, class_labels, n_features=5):\n",
        "    \"\"\"Extract top features for each class from a trained model\"\"\"\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        if hasattr(model, 'coef_'):\n",
        "            # For linear models like Logistic Regression\n",
        "            coefficients = model.coef_[i]\n",
        "            top_indices = coefficients.argsort()[-n_features:][::-1]\n",
        "            top_features = [(feature_names[idx], coefficients[idx]) for idx in top_indices]\n",
        "        else:\n",
        "            # For tree-based models like Random Forest\n",
        "            importances = model.feature_importances_\n",
        "            top_indices = importances.argsort()[-n_features:][::-1]\n",
        "            top_features = [(feature_names[idx], importances[idx]) for idx in top_indices]\n",
        "\n",
        "        print(f\"\\nüîç Top features for '{class_label}':\")\n",
        "        for feature, score in top_features:\n",
        "            print(f\"  {feature}: {score:.3f}\")\n",
        "\n",
        "# Analyze Logistic Regression features\n",
        "print(\"üß† Logistic Regression - Most Important Features:\")\n",
        "get_top_features(lr_model, tfidf_vectorizer, lr_model.classes_)\n",
        "\n",
        "# Analyze Random Forest features (overall importance)\n",
        "print(\"\\nüå≥ Random Forest - Most Important Features (Overall):\")\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "importances = rf_model.feature_importances_\n",
        "top_indices = importances.argsort()[-10:][::-1]\n",
        "\n",
        "print(\"Top 10 most important features:\")\n",
        "for idx in top_indices:\n",
        "    print(f\"  {feature_names[idx]}: {importances[idx]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_2"
      },
      "source": [
        "### ‚úèÔ∏è YOUR TURN - Exercise 2: Comparing All Methods\n",
        "\n",
        "Now let's compare all our sentiment analysis methods on the same test data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "student_exercise_2"
      },
      "outputs": [],
      "source": [
        "#‚úèÔ∏è YOUR TURN: Compare all methods on the same data\n",
        "\n",
        "# 1. Apply the same test set to all models\n",
        "test_texts = X_test.tolist()\n",
        "true_labels = y_test.tolist()\n",
        "\n",
        "# TODO: Get predictions from all methods\n",
        "# (Assumes lr_predictions and rf_predictions are already available from the previous step)\n",
        "vader_predictions = []\n",
        "textblob_predictions = []\n",
        "\n",
        "# TODO: For each text in test_texts, get predictions from VADER and TextBlob\n",
        "for text in test_texts:\n",
        "    # VADER prediction\n",
        "    vader_score = analyze_with_vader(text)['compound']\n",
        "    vader_pred = classify_vader_sentiment(vader_score).lower()\n",
        "    vader_predictions.append(vader_pred)\n",
        "\n",
        "    # TextBlob prediction\n",
        "    textblob_score = analyze_with_textblob(text)['polarity']\n",
        "    textblob_pred = classify_textblob_sentiment(textblob_score).lower()\n",
        "    textblob_predictions.append(textblob_pred)\n",
        "\n",
        "# 2. Calculate final accuracies\n",
        "vader_acc = accuracy_score(true_labels, vader_predictions)\n",
        "textblob_acc = accuracy_score(true_labels, textblob_predictions)\n",
        "lr_acc = accuracy_score(true_labels, lr_predictions)\n",
        "rf_acc = accuracy_score(true_labels, rf_predictions)\n",
        "\n",
        "# 3. Print the high-level comparison results\n",
        "print(\"üèÜ Method Comparison Results:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"VADER Accuracy:          {vader_acc:.3f}\")\n",
        "print(f\"TextBlob Accuracy:       {textblob_acc:.3f}\")\n",
        "print(f\"Logistic Regression:     {lr_acc:.3f}\")\n",
        "print(f\"Random Forest:           {rf_acc:.3f}\")\n",
        "\n",
        "# 4. Analyze and print specific disagreements\n",
        "print(\"\\nüîç Analyzing Disagreements in the Results:\")\n",
        "print(\"=\" * 40)\n",
        "disagreements = 0\n",
        "for i, (text, true_label) in enumerate(zip(test_texts, true_labels)):\n",
        "    predictions = {\n",
        "        'VADER': vader_predictions[i],\n",
        "        'TextBlob': textblob_predictions[i],\n",
        "        'LogReg': lr_predictions[i],\n",
        "        'RandomForest': rf_predictions[i]\n",
        "    }\n",
        "\n",
        "    # Check if there was any disagreement among the predictions\n",
        "    if len(set(predictions.values())) > 1:\n",
        "        disagreements += 1\n",
        "        print(f\"\\nDisagreement #{disagreements}:\")\n",
        "        print(f\"Text:       {text}\")\n",
        "        print(f\"True Label: {true_label}\")\n",
        "        for method, pred in predictions.items():\n",
        "            correct = \"‚úì\" if pred == true_label else \"‚úó\"\n",
        "            print(f\"  -> {method:<15} predicted: {pred:<10} {correct}\")\n",
        "\n",
        "print(f\"\\n\\nüìä Total disagreements found among the models: {disagreements}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02c64aaqjBdO"
      },
      "source": [
        "###**üî¨ Hands-On Analysis Questions**\n",
        "\n",
        "**Answer these based on YOUR specific results above:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS8qbrgujckT"
      },
      "source": [
        "**Q1: Looking at YOUR results above, which method performed best? Was this what you expected?**\n",
        "\n",
        "Your answer based on your results:\n",
        "\n",
        "Your answer here.\n",
        "\n",
        "**Q2: Examine the disagreements YOUR code found. Pick one disagreement and explain why you think the methods disagreed.**\n",
        "\n",
        "Your analysis of a specific disagreement:\n",
        "\n",
        "Your answer here.\n",
        "\n",
        "**Q3: How do the TF-IDF features (from Module 4) help the ML models in a way that the rule-based methods can't access?**\n",
        "\n",
        "Your answer explaining the role of TF-IDF:\n",
        "\n",
        "Your answer here.\n",
        "\n",
        "**Q4: If you had to choose ONE method for a real business application, which would you choose based on YOUR results? Why?**\n",
        "\n",
        "*Your final decision and reasoning. A complete answer should consider the trade-offs by describing a scenario where a rule-based model like VADER might be better, and another scenario where the ML model is the only choice.*\n",
        "\n",
        "Your decision and reasoning here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "audio_section"
      },
      "source": [
        "## Part 3: Speech Emotion Detection\n",
        "\n",
        "### From Audio Preprocessing to Emotion Analysish\n",
        "\n",
        "\n",
        "In Module 3, you learned how to process and clean audio signals. Now, we'll take that a step further. Instead of just cleaning audio, we will extract emotional features from speech signals. The goal is to identify measurable characteristics of a sound wave that correspond to how an emotion is expressed vocally.\n",
        "\n",
        "*   What is being said? (Text Analysis - Parts 1 & 2)\n",
        "*   How is it being said? (Speech Analysis - Part 3)List item\n",
        "\n",
        "We will use the librosa library to analyze audio and extract features like pitch, energy, and tempo, which are crucial for detecting emotion in speech.\n",
        "\n",
        "Key Connection: Audio Preprocessing + Feature Extraction + Machine Learning = Speech Emotion Detection\n",
        "\n",
        "Speech carries emotional information that text alone cannot capture:\n",
        "- **Prosodic features**: Pitch, rhythm, stress patterns\n",
        "- **Voice quality**: Tone, breathiness, tension\n",
        "- **Temporal dynamics**: Speaking rate, pauses\n",
        "\n",
        "Let's build a speech emotion detector using audio signal processing techniques.\n",
        "\n",
        "**Note**: In this lab, we'll simulate audio features since we're working in a text-based environment. In a real application, you would extract these features from actual audio files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "simulate_audio_features"
      },
      "outputs": [],
      "source": [
        "# In a real application, these would be extracted from audio files using librosa\n",
        "\n",
        "# Create sample audio signals that demonstrate emotional speech patterns\n",
        "def create_emotional_audio(emotion, duration=2, sample_rate=22050):\n",
        "    \"\"\"Create audio that simulates emotional speech patterns\"\"\"\n",
        "    t = np.linspace(0, duration, int(sample_rate * duration))\n",
        "\n",
        "    if emotion == 'happy':\n",
        "        # Happy: Higher pitch, more variation, upward intonation\n",
        "        base_freq = 220\n",
        "        pitch_variation = 50 * np.sin(2 * np.pi * 2 * t)\n",
        "        amplitude = 0.7\n",
        "    elif emotion == 'sad':\n",
        "        # Sad: Lower pitch, less variation, downward intonation\n",
        "        base_freq = 150\n",
        "        pitch_variation = 10 * np.sin(2 * np.pi * 0.5 * t)\n",
        "        amplitude = 0.3\n",
        "    elif emotion == 'angry':\n",
        "        # Angry: Variable pitch, harsh quality, higher energy\n",
        "        base_freq = 200\n",
        "        pitch_variation = 80 * np.sin(2 * np.pi * 3 * t) * np.random.normal(1, 0.2, len(t))\n",
        "        amplitude = 0.8\n",
        "    else:  # neutral\n",
        "        # Neutral: Steady pitch, moderate energy\n",
        "        base_freq = 180\n",
        "        pitch_variation = 20 * np.sin(2 * np.pi * 1 * t)\n",
        "        amplitude = 0.5\n",
        "\n",
        "    # Create the audio signal\n",
        "    frequency = base_freq + pitch_variation\n",
        "    audio = amplitude * np.sin(2 * np.pi * frequency * t)\n",
        "\n",
        "    # Add harmonics for more realistic sound\n",
        "    audio += 0.3 * amplitude * np.sin(2 * np.pi * frequency * 2 * t)\n",
        "    audio += 0.1 * amplitude * np.sin(2 * np.pi * frequency * 3 * t)\n",
        "\n",
        "    # Add noise for realism\n",
        "    noise = 0.05 * np.random.normal(0, 1, len(audio))\n",
        "    audio += noise\n",
        "\n",
        "    return audio, sample_rate\n",
        "\n",
        "# Create and play sample audio for each emotion\n",
        "emotions = ['happy', 'sad', 'angry', 'neutral']\n",
        "audio_samples = {}\n",
        "\n",
        "print(\"üéµ Creating Emotional Speech Samples:\")\n",
        "print(\"Listen to how different emotions sound in speech patterns...\\n\")\n",
        "\n",
        "for emotion in emotions:\n",
        "    audio, sr = create_emotional_audio(emotion)\n",
        "    audio_samples[emotion] = (audio, sr)\n",
        "\n",
        "    print(f\"üé≠ {emotion.capitalize()} Audio:\")\n",
        "    display(Audio(audio, rate=sr))\n",
        "    print(f\"   Characteristics: {emotion} speech patterns simulated\")\n",
        "    print()\n",
        "\n",
        "print(\"üí° Connection to Module 3:\")\n",
        "print(\"- We're using the same audio processing concepts (sample rate, frequency).\")\n",
        "print(\"- But now we are extracting EMOTIONAL features instead of just cleaning the audio.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "audio_visualization"
      },
      "source": [
        "### 3.1 Extracting Emotional Features from Audio\n",
        "\n",
        "Now that we have audio signals, we'll use librosa to extract features that quantify the emotional characteristics we just heard. These features include:\n",
        "\n",
        "Pitch: The fundamental frequency of the voice (how high or low it is).\n",
        "\n",
        "* Energy (RMS): The loudness or intensity of the speech.\n",
        "\n",
        "* Tempo: The speed or pace of speech (Beats Per Minute).\n",
        "\n",
        "* Spectral Centroid: Relates to the \"brightness\" of a sound."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize_audio_features"
      },
      "outputs": [],
      "source": [
        "# Extract emotional features from audio (building on Module 3)\n",
        "def extract_emotional_features(audio, sample_rate):\n",
        "    \"\"\"Extract features that indicate emotional state\"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # Prosodic features (how we speak)\n",
        "    pitches, magnitudes = librosa.piptrack(y=audio, sr=sample_rate)\n",
        "    pitch_values = []\n",
        "    for t in range(pitches.shape[1]):\n",
        "        index = magnitudes[:, t].argmax()\n",
        "        pitch = pitches[index, t]\n",
        "        if pitch > 0:\n",
        "            pitch_values.append(pitch)\n",
        "\n",
        "    if pitch_values:\n",
        "        features['pitch_mean'] = np.mean(pitch_values)\n",
        "        features['pitch_std'] = np.std(pitch_values)\n",
        "    else:\n",
        "        features['pitch_mean'] = 0\n",
        "        features['pitch_std'] = 0\n",
        "\n",
        "    # Energy features\n",
        "    rms_energy = librosa.feature.rms(y=audio)[0]\n",
        "    features['energy_mean'] = np.mean(rms_energy)\n",
        "    features['energy_std'] = np.std(rms_energy)\n",
        "\n",
        "    # Spectral features\n",
        "    spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sample_rate)[0]\n",
        "    features['spectral_centroid_mean'] = np.mean(spectral_centroids)\n",
        "\n",
        "    # Tempo\n",
        "    tempo, _ = librosa.beat.beat_track(y=audio, sr=sample_rate)\n",
        "    features['tempo'] = tempo\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features from our emotional audio samples\n",
        "print(\"üîç Extracting Emotional Features from Audio:\")\n",
        "audio_feature_data = []\n",
        "\n",
        "# Create multiple samples per emotion for better analysis\n",
        "for emotion in emotions:\n",
        "    print(f\"\\nProcessing {emotion.capitalize()} emotion samples...\")\n",
        "\n",
        "    for i in range(25):  # 25 samples per emotion for a balanced dataset\n",
        "        audio, sr = create_emotional_audio(emotion, duration=1.5)\n",
        "        features = extract_emotional_features(audio, sr)\n",
        "        features['emotion'] = emotion\n",
        "        audio_feature_data.append(features)\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "df_audio = pd.DataFrame(audio_feature_data)\n",
        "print(f\"\\nüìä Audio emotion dataset created: {len(df_audio)} samples, {len(df_audio.columns)-1} features\")\n",
        "print(\"\\nFirst 5 rows of the feature dataset:\")\n",
        "print(df_audio.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "audio_classification"
      },
      "source": [
        "### 3.2 Visualizing Audio Features\n",
        "\n",
        "Before building a classifier, let's visualize the features we just extracted. This helps us see if there are clear, measurable differences between the emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_audio_classifier"
      },
      "outputs": [],
      "source": [
        "# Visualize key audio features by emotion\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Audio Features by Emotion', fontsize=16)\n",
        "\n",
        "key_features = ['pitch_mean', 'energy_mean', 'tempo', 'spectral_centroid_mean']\n",
        "\n",
        "for i, feature in enumerate(key_features):\n",
        "    row, col = i // 2, i % 2\n",
        "    sns.boxplot(x='emotion', y=feature, data=df_audio, ax=axes[row, col])\n",
        "    axes[row, col].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
        "    axes[row, col].set_xlabel('Emotion')\n",
        "    axes[row, col].set_ylabel('Value')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# Statistical summary\n",
        "print(\"\\nüìä Audio Feature Statistics by Emotion:\")\n",
        "summary_stats = df_audio.groupby('emotion')[key_features].mean()\n",
        "print(summary_stats.round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_3"
      },
      "source": [
        "### ‚úèÔ∏è YOUR TURN - Exercise 3: Train an Audio Emotion Classifier\n",
        "\n",
        "Now it's your turn to use these features to train a machine learning model. Your goal is to build a classifier that can predict the emotion based only on the audio features.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "1. Prepare the data by separating features (X) and labels (y).\n",
        "\n",
        "2. Split the data into training and testing sets. Remember to use stratify to keep the emotion distribution even.\n",
        "\n",
        "3. Scale the features using StandardScaler. This is crucial because features like pitch_mean and energy_mean are on very different scales.\n",
        "\n",
        "4. Train a RandomForestClassifier on the scaled training data.\n",
        "\n",
        "5. Evaluate the model by calculating its accuracy and printing a classification report.\n",
        "\n",
        "6. Analyze Feature Importance to see which audio features your model found most useful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "student_exercise_3"
      },
      "outputs": [],
      "source": [
        "# ‚úèÔ∏è YOUR TURN: Train and analyze an audio emotion classifier\n",
        "\n",
        "# 1. Prepare the data\n",
        "# TODO: Define your feature columns (all columns except 'emotion')\n",
        "feature_columns = [col for col in df_audio.columns if col != 'emotion']\n",
        "X_audio = df_audio[feature_columns]\n",
        "y_audio = df_audio['emotion']\n",
        "\n",
        "# 2. Split and scale\n",
        "# TODO: Split data into training and testing sets (test_size=0.3, random_state=42)\n",
        "X_train_audio, X_test_audio, y_train_audio, y_test_audio = train_test_split(\n",
        "    X_audio, y_audio, test_size=0.3, random_state=42, stratify=y_audio\n",
        ")\n",
        "\n",
        "# TODO: Initialize a StandardScaler and scale your training and test data\n",
        "scaler = StandardScaler()\n",
        "X_train_audio_scaled = scaler.fit_transform(X_train_audio)\n",
        "X_test_audio_scaled = scaler.transform(X_test_audio)\n",
        "\n",
        "# 3. Train classifier\n",
        "# TODO: Initialize and train a RandomForestClassifier\n",
        "audio_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "audio_classifier.fit(X_train_audio_scaled, y_train_audio)\n",
        "\n",
        "# 4. Evaluate\n",
        "# TODO: Make predictions on the scaled test data and calculate the accuracy\n",
        "audio_predictions = audio_classifier.predict(X_test_audio_scaled)\n",
        "audio_accuracy = accuracy_score(y_test_audio, audio_predictions)\n",
        "\n",
        "print(\"üéµ YOUR Audio Emotion Classification Results:\")\n",
        "print(f\"Accuracy: {audio_accuracy:.3f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_audio, audio_predictions))\n",
        "\n",
        "# 5. Feature importance analysis\n",
        "# TODO: Get feature importances from the trained model\n",
        "importances = audio_classifier.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': feature_columns,\n",
        "    'importance': importances\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nüîç Most Important Audio Features in YOUR Model:\")\n",
        "print(feature_importance_df.head(5))\n",
        "\n",
        "# 6. Visualize feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance_df.head(5), palette='viridis')\n",
        "plt.title('Top 5 Most Important Audio Features')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Audio Feature')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdPScLXszbr5"
      },
      "source": [
        "##**üéß Audio Analysis Questions**##\n",
        "\n",
        "###**Answer based on YOUR specific results and visualizations above:**###\n",
        "\n",
        "**Q1: Looking at YOUR feature importance results, which audio features were most important for distinguishing emotions in your model?**\n",
        "\n",
        "Your answer based on your results:\n",
        "\n",
        "**Q2: Examining YOUR boxplots above, which emotion shows the most distinctive pattern? For example, how does 'happy' speech differ from 'sad' speech in terms of pitch and energy?**\n",
        "\n",
        "Your observation of your visualizations:\n",
        "\n",
        "**Q3: How does YOUR audio classifier's accuracy compare to the text-based methods from Part 2? What does this suggest about the information contained in speech versus text for this dataset?**\n",
        "\n",
        "Your comparison and interpretation:\n",
        "\n",
        "**Q4: If you were analyzing real human speech, what additional challenges might you face that our simulated data doesn't capture? (Think about background noise, different speakers, accents, etc.)**\n",
        "\n",
        "Your answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "multimodal_section"
      },
      "source": [
        "## **Part 4: Multimodal Emotion Analysis**\n",
        "\n",
        "Combining All Previous Modules: The Ultimate Integration\n",
        "\n",
        "This is the final and most important part of our lab. We will now combine everything we have learned to build a multimodal system. A multimodal system is smarter because it uses more than one type of data‚Äîin our case, both text and audio.\n",
        "\n",
        "* **Module 2 (Preprocessing): We'll need clean text.**\n",
        "\n",
        "* **Module 3 (Audio): We'll use the emotional features we just extracted from audio signals.**\n",
        "\n",
        "* **Module 4 (Text Representation): We will use TF-IDF to convert text into numbers.**\n",
        "\n",
        "* **Modules 5 & 6 (Linguistics): The text portion of our model implicitly relies on the linguistic patterns that TF-IDF captures.**\n",
        "\n",
        "* **Machine Learning: We will fuse these different data sources together to make a single, more accurate prediction.**\n",
        "\n",
        "* **The goal is to see if a model that can both read the words and hear the tone of voice performs better than a model that can only do one or the other.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_multimodal_data"
      },
      "outputs": [],
      "source": [
        "# Create multimodal dataset (text + corresponding audio)\n",
        "multimodal_texts = {\n",
        "    'happy': [\n",
        "        \"I'm so excited about this opportunity!\",\n",
        "        \"This is absolutely wonderful news!\",\n",
        "        \"I love spending time with my family!\",\n",
        "        \"What a beautiful day it is today!\",\n",
        "        \"I'm thrilled to be here with you all!\"\n",
        "    ],\n",
        "    'sad': [\n",
        "        \"I'm feeling really down today.\",\n",
        "        \"This situation makes me very sad.\",\n",
        "        \"I miss my old friends so much.\",\n",
        "        \"Everything seems to be going wrong.\",\n",
        "        \"I feel so lonely and isolated.\"\n",
        "    ],\n",
        "    'angry': [\n",
        "        \"This is completely unacceptable!\",\n",
        "        \"I'm furious about this decision!\",\n",
        "        \"How dare you treat me this way!\",\n",
        "        \"This makes me so angry and frustrated!\",\n",
        "        \"I can't believe this is happening!\"\n",
        "    ],\n",
        "    'neutral': [\n",
        "        \"The meeting is scheduled for tomorrow.\",\n",
        "        \"Please submit your report by Friday.\",\n",
        "        \"The weather forecast shows rain.\",\n",
        "        \"I need to buy groceries later.\",\n",
        "        \"The train arrives at 3:30 PM.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create multimodal dataset\n",
        "multimodal_data = []\n",
        "print(\"üé≠ Creating Multimodal Dataset (Text + Audio):\")\n",
        "\n",
        "for emotion, texts in multimodal_texts.items():\n",
        "    print(f\"\\nüéµ {emotion.capitalize()} examples:\")\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        # Generate corresponding audio using the function from Part 3\n",
        "        audio, sr = create_emotional_audio(emotion, duration=2.0)\n",
        "\n",
        "        # Extract audio features using the function from Part 3\n",
        "        audio_features = extract_emotional_features(audio, sr)\n",
        "\n",
        "        # Store the text, the audio features, and the emotion label together\n",
        "        multimodal_data.append({\n",
        "            'text': text,\n",
        "            'audio_features': audio_features,\n",
        "            'emotion': emotion\n",
        "        })\n",
        "\n",
        "        print(f\"  Text: {text}\")\n",
        "        if i == 0:  # Play the first audio sample for demonstration\n",
        "            print(f\"  Listen to its corresponding audio:\")\n",
        "            display(Audio(audio, rate=sr))\n",
        "\n",
        "print(f\"\\nüìä Multimodal Dataset Created: {len(multimodal_data)} samples\")\n",
        "print(\"Each sample now contains both text and its corresponding emotional audio features.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "multimodal_features"
      },
      "source": [
        "### **4.1 Fusing Text and Audio Features**\n",
        "\n",
        "To build a multimodal model, we need to combine our numerical representations of text (TF-IDF vectors) and audio (feature sets) into a single input for our classifier. This process is called feature fusion. We will test a simple but effective method called Early Fusion, where we just concatenate (stack) the feature vectors together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extract_multimodal_features"
      },
      "outputs": [],
      "source": [
        "# ‚úèÔ∏è Main Analysis: Build and Compare Multimodal Emotion Detection Systems\n",
        "\n",
        "# 1. Extract features from both modalities\n",
        "multimodal_texts_list = [sample['text'] for sample in multimodal_data]\n",
        "multimodal_emotions = [sample['emotion'] for sample in multimodal_data]\n",
        "\n",
        "# Text features (Module 4 concepts)\n",
        "multimodal_tfidf = TfidfVectorizer(max_features=50, stop_words='english')\n",
        "text_features = multimodal_tfidf.fit_transform(multimodal_texts_list).toarray()\n",
        "\n",
        "# Audio features (Module 3 concepts)\n",
        "# We need to extract the audio features from our list of dictionaries\n",
        "audio_features_df = pd.DataFrame([d['audio_features'] for d in multimodal_data])\n",
        "audio_features = audio_features_df.values\n",
        "audio_scaler = StandardScaler()\n",
        "audio_features_scaled = audio_scaler.fit_transform(audio_features)\n",
        "\n",
        "# 2. Create the fused feature set by combining text and audio\n",
        "# This is our \"Early Fusion\" approach\n",
        "fused_features = np.concatenate([text_features, audio_features_scaled], axis=1)\n",
        "\n",
        "print(f\"Shape of Text Features: {text_features.shape}\")\n",
        "print(f\"Shape of Audio Features: {audio_features_scaled.shape}\")\n",
        "print(f\"Shape of Fused Features: {fused_features.shape}\")\n",
        "\n",
        "\n",
        "# 3. Train and evaluate different approaches\n",
        "approaches = {\n",
        "    'Text Only': text_features,\n",
        "    'Audio Only': audio_features_scaled,\n",
        "    'Multimodal (Fused)': fused_features\n",
        "}\n",
        "\n",
        "results = {}\n",
        "print(\"\\nüé≠ Comparing Model Performance:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for approach_name, features in approaches.items():\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, multimodal_emotions, test_size=0.3, random_state=42, stratify=multimodal_emotions\n",
        "    )\n",
        "\n",
        "    # Train a Logistic Regression classifier\n",
        "    classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    predictions = classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    results[approach_name] = accuracy\n",
        "\n",
        "    print(f\"{approach_name:20}: Accuracy = {accuracy:.3f}\")\n",
        "\n",
        "# 4. Analyze the improvement from fusion\n",
        "best_single_modality_acc = max(results['Text Only'], results['Audio Only'])\n",
        "multimodal_acc = results['Multimodal (Fused)']\n",
        "improvement = multimodal_acc - best_single_modality_acc\n",
        "\n",
        "print(\"\\nüìà Analysis of Multimodal Improvement:\")\n",
        "print(f\"Best Single-Modality Accuracy: {best_single_modality_acc:.3f}\")\n",
        "print(f\"Multimodal (Fused) Accuracy:   {multimodal_acc:.3f}\")\n",
        "print(f\"Improvement from Fusion:       {improvement:+.3f}\")\n",
        "\n",
        "if improvement > 0.01: # Check for a meaningful improvement\n",
        "    print(\"\\n‚úÖ Conclusion: Multimodal fusion provided a clear improvement over using just text or audio alone!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Conclusion: Multimodal fusion did not provide a significant improvement in this case.\")\n",
        "\n",
        "print(\"\\nüîç Analyze the results above to answer the final questions below.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "multimodal_training"
      },
      "source": [
        "##üéØ YOUR TURN - Final Comprehensive Reflection##\n",
        "\n",
        "This is the final part of the lab. Based on everything you have built and the results you see above, answer the following questions.\n",
        "\n",
        "**Q1: Integration Question - How did concepts from each previous module contribute to the final multimodal system in Part 4?**\n",
        "\n",
        "* Module 2 (Preprocessing): Your answer...\n",
        "\n",
        "* Module 3 (Audio): Your answer...\n",
        "\n",
        "* Module 4 (Text Representation): Your answer...\n",
        "\n",
        "**Q2: Results Analysis Question - Based on YOUR specific results, did the multimodal (fused) model perform better than the single-modality (Text Only, Audio Only) models? Why do you think this happened?**\n",
        "\n",
        "Your analysis of your results:\n",
        "\n",
        "**Q3: Real-World Application Question - Imagine you were building an AI assistant for a call center to detect customer frustration. Which model would you choose (Text Only, Audio Only, or Multimodal)? Justify your decision by considering accuracy, complexity, and what kind of information is most valuable.**\n",
        "\n",
        "Your business decision and reasoning:\n",
        "\n",
        "**Q4: Bias and Ethics Question - What is the biggest ethical risk of deploying an emotion detection system like this? Describe one potential type of bias (e.g., cultural, gender, age) and explain how it might cause problems.**\n",
        "\n",
        "Your answer on ethics and bias:\n",
        "\n",
        "**Q5: Personal Experience Question - What was the most challenging part of this lab for YOU personally, and what was the most surprising or interesting discovery you made?**\n",
        "\n",
        "Your personal experience and discoveries:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-gTLixO62dE"
      },
      "source": [
        "## üéâ Lab Conclusion\n",
        "\n",
        "### What You've Accomplished\n",
        "\n",
        "Congratulations! You've successfully:\n",
        "\n",
        "1. **‚úÖ Connected all previous modules** to build a comprehensive emotion analysis system\n",
        "2. **‚úÖ Built rule-based sentiment analyzers** using VADER and TextBlob\n",
        "3. **‚úÖ Created machine learning classifiers** using TF-IDF and scikit-learn\n",
        "4. **‚úÖ Developed speech emotion detection** using audio feature extraction\n",
        "5. **‚úÖ Implemented multimodal fusion** combining text and audio analysis\n",
        "6. **‚úÖ Analyzed bias and ethical considerations** in emotion detection systems\n",
        "\n",
        "### Key Insights from Your Journey\n",
        "\n",
        "- **Integration is powerful**: Combining concepts from all modules creates more robust systems\n",
        "- **Different approaches have different strengths**: Rule-based, ML, and multimodal each excel in different scenarios\n",
        "- **Context matters**: The same words can convey different emotions depending on how they're spoken\n",
        "- **Bias is real**: Emotion detection systems can perpetuate societal biases and must be carefully evaluated\n",
        "- **Practical considerations**: Real-world deployment requires balancing accuracy, interpretability, and computational cost\n",
        "\n",
        "### Looking Forward\n",
        "\n",
        "This lab represents the culmination of your foundational NLP learning. In upcoming modules, you'll learn about:\n",
        "- **Neural networks** for more sophisticated emotion analysis\n",
        "- **Deep learning architectures** that can learn complex patterns\n",
        "- **Transformer models** that understand context even better\n",
        "- **Large language models** that can perform emotion analysis with minimal training\n",
        "\n",
        "### Final Thought\n",
        "\n",
        "Remember: The goal isn't just to build accurate models, but to build **fair, ethical, and beneficial systems** that respect human dignity and promote positive outcomes for all users.\n",
        "\n",
        "**Great work completing this comprehensive emotion analysis lab! You now have hands-on experience with the full spectrum of emotion detection approaches, from rule-based methods to multimodal machine learning systems.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "### üöÄ Next Steps and Further Learning\n",
        "\n",
        "To continue developing your emotion analysis skills:\n",
        "\n",
        "1. **Explore real datasets**: Try your techniques on actual emotion datasets like IEMOCAP or RAVDESS ( You can download  RADVES from Kaggle)\n",
        "2. **Advanced audio features**: Learn about more sophisticated audio features like prosodic contours and voice quality measures\n",
        "3. **Deep learning approaches**: Explore neural networks for emotion detection (coming in future modules!)\n",
        "4. **Multimodal architectures**: Study attention mechanisms and more sophisticated fusion techniques\n",
        "5. **Bias mitigation**: Research techniques for reducing bias in emotion detection systems\n",
        "\n",
        "### üìö Additional Resources:\n",
        "\n",
        "- **VADER Documentation**: https://github.com/cjhutto/vaderSentiment\n",
        "- **TextBlob Documentation**: https://textblob.readthedocs.io/\n",
        "- **Librosa for Audio Analysis**: https://librosa.org/\n",
        "- **Emotion Recognition Research**: Search for \"multimodal emotion recognition\" papers\n",
        "- **Bias in AI**: Research on fairness and bias in machine learning systems\n",
        "\n",
        "---\n",
        "\n",
        "**Great work completing this comprehensive emotion analysis lab! You now have hands-on experience with multiple approaches to understanding emotions in text and speech. These skills will be valuable as we continue exploring advanced NLP techniques in upcoming modules.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
